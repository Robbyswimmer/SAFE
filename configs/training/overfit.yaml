# Overfitting Training Configuration
# For testing model capacity and retention behavior under overfitting
name: "overfit"
description: "High epoch training on tiny datasets to test overfitting and retention"

# Training parameters
num_epochs: 20  # Many epochs to force overfitting
batch_size: 4
gradient_accumulation_steps: 1

# Learning rates - slightly higher for faster overfitting
learning_rate_projector: 2e-4
learning_rate_adapter: 1e-4
weight_decay: 0.001  # Lower weight decay to allow overfitting
warmup_steps: 25
max_grad_norm: 1.0

# Evaluation settings - minimal evaluation during overfitting tests
eval_every_n_epochs: 5  # Less frequent evaluation
max_eval_batches: 10  # Small but reasonable evaluation
validation_frequency: 5

# Loss weights
audio_loss_weight: 1.0
retention_loss_weight: 1.0
distillation_temperature: 3.0
fisher_weight: 0.05  # Lower Fisher weight for overfitting test
retention_tolerance: 0.02  # 2% tolerance during overfitting
audio_contrastive_weight: 0.0
audio_contrastive_temperature: 0.07
audio_contrastive_answer_max_length: 48
audio_contrastive_metric_weight: 0.0
audio_contrastive_negative_threshold: 0.0
audio_contrastive_max_negatives: 0
audio_num_beams: 2
audio_num_return_sequences: 2
audio_num_samples: 2
audio_sample_top_p: 0.9
audio_sample_temperature: 0.9
audio_length_penalty: 0.9
audio_rerank_with_clap: false
audio_rerank_logprob_weight: 1.0
audio_rerank_clap_weight: 0.0
audio_rerank_ngram_weight: 0.0
audio_rerank_coverage_weight: 0.0
audio_rerank_cider_weight: 0.0
audio_rerank_spider_weight: 0.0
audio_rerank_tag_vocab: null
audio_rerank_num_tags: 0
audio_rerank_tag_weight: 0.0

# Logging and saving
logging_steps: 10
save_steps: 200
early_stopping_patience: 20  # Allow more patience for overfitting

# Performance
expected_runtime_minutes: 15
compute_requirements: "low"
