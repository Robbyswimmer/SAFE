# Retention Training Configuration  
# For focused retention validation and baseline studies
name: "retention"
description: "Training focused on retention validation and baseline comparison"

# Training parameters
num_epochs: 5
batch_size: 4
gradient_accumulation_steps: 2

# Learning rates - conservative for retention study
learning_rate_projector: 1e-4
learning_rate_adapter: 5e-5
weight_decay: 0.01
warmup_steps: 50
max_grad_norm: 1.0

# Evaluation settings - frequent for retention monitoring
eval_every_n_epochs: 1
max_eval_batches: 25  # Reasonable evaluation size
validation_frequency: 1

# Loss weights - emphasis on retention with exact phrasing
audio_loss_weight: 0.8  # Slightly lower audio weight
retention_loss_weight: 1.2  # Higher retention weight
distillation_temperature: 3.0
fisher_weight: 0.15  # Higher Fisher weight for retention
retention_tolerance: 0.005  # Strict 0.5% tolerance
audio_contrastive_weight: 0.0  # DISABLED: Temporarily off to focus on reranking & SCST
audio_contrastive_temperature: 0.07
audio_contrastive_answer_max_length: 64
audio_contrastive_metric_weight: 0.2
audio_contrastive_negative_threshold: 0.3
audio_contrastive_max_negatives: 16

# Generation parameters - increased candidates for better reranking
audio_num_beams: 5  # Scaled for retention config (smaller than full)
audio_num_return_sequences: 5
audio_num_samples: 5  # Total 10 candidates (balanced for retention study)
audio_sample_top_p: 0.9
audio_sample_temperature: 0.85
audio_length_penalty: 0.9

# Reranking weights - aggressive tuning for exact phrase matching
# NOTE: log-prob is negative (-5 to -1), so must be heavily down-weighted
audio_rerank_with_clap: true
audio_rerank_logprob_weight: 0.05  # CRITICAL: Reduced from 1.0 to prevent negative log-prob dominance
audio_rerank_clap_weight: 0.3
audio_rerank_ngram_weight: 1.2  # Increased from 0.6 - strong n-gram emphasis
audio_rerank_coverage_weight: 0.1
audio_rerank_cider_weight: 1.5  # Increased from 0.7 - strongest signal for exact phrasing
audio_rerank_spider_weight: 0.25  # Increased from 0.2
audio_rerank_tag_vocab: "configs/tags/audio_tag_vocab.txt"
audio_rerank_num_tags: 4
audio_rerank_tag_weight: 0.2

# Logging and saving
logging_steps: 25
save_steps: 100
early_stopping_patience: 5  # Early stop if retention degrades

# Performance  
expected_runtime_minutes: 30
compute_requirements: "moderate"

# Retention-specific settings
compute_baseline_metrics: true
track_retention_per_epoch: true
retention_failure_threshold: 0.02  # Stop if retention drops >2%

# SCST Fine-tuning (auto-triggers after plateau for direct metric optimization)
enable_scst_finetune: true
scst_epochs: 1  # Conservative for retention study
scst_learning_rate: 5e-6  # Low LR for policy gradient stability
scst_num_samples: 2  # Scaled for retention config
scst_sample_top_p: 0.9
scst_sample_temperature: 0.85
scst_reward_metric: "cider"  # Optimize for CIDEr (exact phrase matching)
scst_patience_epochs: 2  # Trigger after 2 epochs without improvement
scst_improvement_threshold: 0.001  # 0.1% improvement required to reset patience
